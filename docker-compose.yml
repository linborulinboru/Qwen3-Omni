version: '3.8'

services:
  qwen3-omni-captioner:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen3-omni-app:latest
    container_name: qwen3-omni-captioner
    ports:
      - "8901:8901"
    volumes:
      - ./app/Qwen/Qwen3-Omni-30B-A3B-Captioner:/data/models/Qwen3-Omni-30B-A3B-Captioner:ro
      - ./web_demo_captioner.py:/app/web_demo_captioner.py:ro
      - ./app/inputs:/app/inputs
      - ./app/outputs:/app/outputs
      - ./app/temp:/app/temp
    working_dir: /app
    environment:
      - VLLM_USE_V1=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_LAUNCH_BLOCKING=0
    command: >
      python3 web_demo_captioner.py
      --use-transformers
      --flash-attn2
      --checkpoint-path /data/models/Qwen3-Omni-30B-A3B-Captioner
      --server-name 0.0.0.0
      --server-port 8901
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '32gb'
    stdin_open: true
    tty: true
    profiles:
      - captioner

  qwen3-omni-captioner-awq:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen3-omni-app:latest
    container_name: qwen3-omni-captioner-awq
    ports:
      - "8901:8901"
    volumes:
      - ./app/Qwen/Qwen3-Omni-30B-A3B-Captioner-AWQ-8bit:/data/models/Qwen3-Omni-30B-A3B-Captioner-AWQ-8bit:ro
      - ./web_captioner_awq.py:/app/web_captioner_awq.py:ro
      - ./app/inputs:/app/inputs
      - ./app/outputs:/app/outputs
      - ./app/temp:/app/temp
    working_dir: /app
    environment:
      - VLLM_USE_V1=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_LAUNCH_BLOCKING=0
    command: >
      python3 web_captioner_awq.py
      --use-transformers
      --flash-attn2
      --checkpoint-path /data/models/Qwen3-Omni-30B-A3B-Captioner-AWQ-8bit
      --server-name 0.0.0.0
      --server-port 8901
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '32gb'
    stdin_open: true
    tty: true
    profiles:
      - captioner-awq

  qwen3-omni-demo-awq:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen3-omni-app:latest
    container_name: qwen3-omni-demo-awq
    ports:
      - "8902:8901"
    volumes:
      - ./app/Qwen/Qwen3-Omni-30B-A3B-Instruct-AWQ-8bit:/data/models/Qwen3-Omni-30B-A3B-Instruct-AWQ-8bit:ro
      - ./web_demo_awq.py:/app/web_demo_awq.py:ro
      - ./app/inputs:/app/inputs
      - ./app/outputs:/app/outputs
      - ./app/temp:/app/temp
    working_dir: /app
    environment:
      - VLLM_USE_V1=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_LAUNCH_BLOCKING=0
    command: >
      python3 web_demo_awq.py
      --use-transformers
      --flash-attn2
      --checkpoint-path /data/models/Qwen3-Omni-30B-A3B-Instruct-AWQ-8bit
      --server-name 0.0.0.0
      --server-port 8901
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '32gb'
    stdin_open: true
    tty: true
    profiles:
      - demo-awq
